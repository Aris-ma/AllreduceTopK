# roberta:
bucket 0 contains 4 tensors,in the bucket, the shape of the 0th tensor is torch.Size([2])
bucket 0 contains 4 tensors,in the bucket, the shape of the 1th tensor is torch.Size([2, 768])
bucket 0 contains 4 tensors,in the bucket, the shape of the 2th tensor is torch.Size([768])
bucket 0 contains 4 tensors,in the bucket, the shape of the 3th tensor is torch.Size([768, 768])

bucket 2 contains 16 tensors,in the bucket, the shape of the 0th tensor is torch.Size([768])
bucket 2 contains 16 tensors,in the bucket, the shape of the 1th tensor is torch.Size([768])
bucket 2 contains 16 tensors,in the bucket, the shape of the 2th tensor is torch.Size([768])
bucket 2 contains 16 tensors,in the bucket, the shape of the 3th tensor is torch.Size([768, 3072])
bucket 2 contains 16 tensors,in the bucket, the shape of the 4th tensor is torch.Size([3072])
bucket 2 contains 16 tensors,in the bucket, the shape of the 5th tensor is torch.Size([3072, 768])
bucket 2 contains 16 tensors,in the bucket, the shape of the 6th tensor is torch.Size([768])
bucket 2 contains 16 tensors,in the bucket, the shape of the 7th tensor is torch.Size([768])
bucket 2 contains 16 tensors,in the bucket, the shape of the 8th tensor is torch.Size([768])
bucket 2 contains 16 tensors,in the bucket, the shape of the 9th tensor is torch.Size([768, 768])
bucket 2 contains 16 tensors,in the bucket, the shape of the 10th tensor is torch.Size([768])
bucket 2 contains 16 tensors,in the bucket, the shape of the 11th tensor is torch.Size([768, 768])
bucket 2 contains 16 tensors,in the bucket, the shape of the 12th tensor is torch.Size([768])
bucket 2 contains 16 tensors,in the bucket, the shape of the 13th tensor is torch.Size([768, 768])
bucket 2 contains 16 tensors,in the bucket, the shape of the 14th tensor is torch.Size([768])
bucket 2 contains 16 tensors,in the bucket, the shape of the 15th tensor is torch.Size([768, 768])

bucket 13 contains 5 tensors,in the bucket, the shape of the 0th tensor is torch.Size([768])
bucket 13 contains 5 tensors,in the bucket, the shape of the 1th tensor is torch.Size([768])
bucket 13 contains 5 tensors,in the bucket, the shape of the 2th tensor is torch.Size([514, 768])
bucket 13 contains 5 tensors,in the bucket, the shape of the 3th tensor is torch.Size([1, 768])
bucket 13 contains 5 tensors,in the bucket, the shape of the 4th tensor is torch.Size([50265, 768])





# resnet18:
[Rank 1] conv1.weight: torch.Size([64, 3, 3, 3])
[Rank 1] bn1.weight: torch.Size([64])
[Rank 1] bn1.bias: torch.Size([64])

[Rank 1] layer1.0.conv1.weight: torch.Size([64, 64, 3, 3])
[Rank 1] layer1.0.bn1.weight: torch.Size([64])
[Rank 1] layer1.0.bn1.bias: torch.Size([64])
[Rank 1] layer1.0.conv2.weight: torch.Size([64, 64, 3, 3])
[Rank 1] layer1.0.bn2.weight: torch.Size([64])
[Rank 1] layer1.0.bn2.bias: torch.Size([64])
[Rank 1] layer1.1.conv1.weight: torch.Size([64, 64, 3, 3])
[Rank 1] layer1.1.bn1.weight: torch.Size([64])
[Rank 1] layer1.1.bn1.bias: torch.Size([64])
[Rank 1] layer1.1.conv2.weight: torch.Size([64, 64, 3, 3])
[Rank 1] layer1.1.bn2.weight: torch.Size([64])
[Rank 1] layer1.1.bn2.bias: torch.Size([64])

[Rank 1] layer2.0.conv1.weight: torch.Size([128, 64, 3, 3])
[Rank 1] layer2.0.bn1.weight: torch.Size([128])
[Rank 1] layer2.0.bn1.bias: torch.Size([128])
[Rank 1] layer2.0.conv2.weight: torch.Size([128, 128, 3, 3])
[Rank 1] layer2.0.bn2.weight: torch.Size([128])
[Rank 1] layer2.0.bn2.bias: torch.Size([128])
[Rank 1] layer2.0.shortcut.0.weight: torch.Size([128, 64, 1, 1])
[Rank 1] layer2.0.shortcut.1.weight: torch.Size([128])
[Rank 1] layer2.0.shortcut.1.bias: torch.Size([128])
[Rank 1] layer2.1.conv1.weight: torch.Size([128, 128, 3, 3])
[Rank 1] layer2.1.bn1.weight: torch.Size([128])
[Rank 1] layer2.1.bn1.bias: torch.Size([128])
[Rank 1] layer2.1.conv2.weight: torch.Size([128, 128, 3, 3])
[Rank 1] layer2.1.bn2.weight: torch.Size([128])
[Rank 1] layer2.1.bn2.bias: torch.Size([128])

[Rank 1] layer3.0.conv1.weight: torch.Size([256, 128, 3, 3])
[Rank 1] layer3.0.bn1.weight: torch.Size([256])
[Rank 1] layer3.0.bn1.bias: torch.Size([256])
[Rank 1] layer3.0.conv2.weight: torch.Size([256, 256, 3, 3])
[Rank 1] layer3.0.bn2.weight: torch.Size([256])
[Rank 1] layer3.0.bn2.bias: torch.Size([256])
[Rank 1] layer3.0.shortcut.0.weight: torch.Size([256, 128, 1, 1])
[Rank 1] layer3.0.shortcut.1.weight: torch.Size([256])
[Rank 1] layer3.0.shortcut.1.bias: torch.Size([256])
[Rank 1] layer3.1.conv1.weight: torch.Size([256, 256, 3, 3])
[Rank 1] layer3.1.bn1.weight: torch.Size([256])
[Rank 1] layer3.1.bn1.bias: torch.Size([256])
[Rank 1] layer3.1.conv2.weight: torch.Size([256, 256, 3, 3])
[Rank 1] layer3.1.bn2.weight: torch.Size([256])
[Rank 1] layer3.1.bn2.bias: torch.Size([256])

[Rank 1] layer4.0.conv1.weight: torch.Size([512, 256, 3, 3])
[Rank 1] layer4.0.bn1.weight: torch.Size([512])
[Rank 1] layer4.0.bn1.bias: torch.Size([512])
[Rank 1] layer4.0.conv2.weight: torch.Size([512, 512, 3, 3])
[Rank 1] layer4.0.bn2.weight: torch.Size([512])
[Rank 1] layer4.0.bn2.bias: torch.Size([512])
[Rank 1] layer4.0.shortcut.0.weight: torch.Size([512, 256, 1, 1])
[Rank 1] layer4.0.shortcut.1.weight: torch.Size([512])
[Rank 1] layer4.0.shortcut.1.bias: torch.Size([512])
[Rank 1] layer4.1.conv1.weight: torch.Size([512, 512, 3, 3])
[Rank 1] layer4.1.bn1.weight: torch.Size([512])
[Rank 1] layer4.1.bn1.bias: torch.Size([512])
[Rank 1] layer4.1.conv2.weight: torch.Size([512, 512, 3, 3])
[Rank 1] layer4.1.bn2.weight: torch.Size([512])
[Rank 1] layer4.1.bn2.bias: torch.Size([512])

[Rank 1] linear.weight: torch.Size([10, 512])
[Rank 1] linear.bias: torch.Size([10])




# llama_130m
[Rank 0] model.layers.0.self_attn.q_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.0.self_attn.k_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.0.self_attn.v_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.0.self_attn.o_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.0.mlp.gate_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.0.mlp.down_proj.weight: torch.Size([768, 2048])
[Rank 0] model.layers.0.mlp.up_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.0.input_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.0.post_attention_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.1.self_attn.q_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.1.self_attn.k_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.1.self_attn.v_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.1.self_attn.o_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.1.mlp.gate_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.1.mlp.down_proj.weight: torch.Size([768, 2048])
[Rank 0] model.layers.1.mlp.up_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.1.input_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.1.post_attention_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.2.self_attn.q_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.2.self_attn.k_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.2.self_attn.v_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.2.self_attn.o_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.2.mlp.gate_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.2.mlp.down_proj.weight: torch.Size([768, 2048])
[Rank 0] model.layers.2.mlp.up_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.2.input_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.2.post_attention_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.3.self_attn.q_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.3.self_attn.k_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.3.self_attn.v_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.3.self_attn.o_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.3.mlp.gate_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.3.mlp.down_proj.weight: torch.Size([768, 2048])
[Rank 0] model.layers.3.mlp.up_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.3.input_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.3.post_attention_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.4.self_attn.q_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.4.self_attn.k_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.4.self_attn.v_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.4.self_attn.o_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.4.mlp.gate_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.4.mlp.down_proj.weight: torch.Size([768, 2048])
[Rank 0] model.layers.4.mlp.up_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.4.input_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.4.post_attention_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.5.self_attn.q_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.5.self_attn.k_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.5.self_attn.v_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.5.self_attn.o_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.5.mlp.gate_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.5.mlp.down_proj.weight: torch.Size([768, 2048])
[Rank 0] model.layers.5.mlp.up_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.5.input_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.5.post_attention_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.6.self_attn.q_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.6.self_attn.k_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.6.self_attn.v_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.6.self_attn.o_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.6.mlp.gate_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.6.mlp.down_proj.weight: torch.Size([768, 2048])
[Rank 0] model.layers.6.mlp.up_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.6.input_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.6.post_attention_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.7.self_attn.q_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.7.self_attn.k_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.7.self_attn.v_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.7.self_attn.o_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.7.mlp.gate_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.7.mlp.down_proj.weight: torch.Size([768, 2048])
[Rank 0] model.layers.7.mlp.up_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.7.input_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.7.post_attention_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.8.self_attn.q_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.8.self_attn.k_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.8.self_attn.v_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.8.self_attn.o_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.8.mlp.gate_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.8.mlp.down_proj.weight: torch.Size([768, 2048])
[Rank 0] model.layers.8.mlp.up_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.8.input_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.8.post_attention_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.9.self_attn.q_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.9.self_attn.k_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.9.self_attn.v_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.9.self_attn.o_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.9.mlp.gate_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.9.mlp.down_proj.weight: torch.Size([768, 2048])
[Rank 0] model.layers.9.mlp.up_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.9.input_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.9.post_attention_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.10.self_attn.q_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.10.self_attn.k_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.10.self_attn.v_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.10.self_attn.o_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.10.mlp.gate_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.10.mlp.down_proj.weight: torch.Size([768, 2048])
[Rank 0] model.layers.10.mlp.up_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.10.input_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.10.post_attention_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.11.self_attn.q_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.11.self_attn.k_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.11.self_attn.v_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.11.self_attn.o_proj.weight: torch.Size([768, 768])
[Rank 0] model.layers.11.mlp.gate_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.11.mlp.down_proj.weight: torch.Size([768, 2048])
[Rank 0] model.layers.11.mlp.up_proj.weight: torch.Size([2048, 768])
[Rank 0] model.layers.11.input_layernorm.weight: torch.Size([768])
[Rank 0] model.layers.11.post_attention_layernorm.weight: torch.Size([768])
[Rank 0] model.norm.weight: torch.Size([768])
[Rank 0] lm_head.weight: torch.Size([32000, 768])